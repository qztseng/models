{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.3\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from io import BytesIO\n",
    "# import tarfile\n",
    "# import tempfile\n",
    "# from six.moves import urllib\n",
    "\n",
    "# from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from deeplab.core import resnet_v1_beta\n",
    "from deeplab.core import feature_extractor\n",
    "from deeplab import common\n",
    "from deeplab import model\n",
    "\n",
    "from deeplab.core import utils\n",
    "from deeplab.core import conv2d_ws\n",
    "# from tensorflow.contrib.slim.nets import resnet_utils\n",
    "import functools\n",
    "from tensorflow.contrib import slim as contrib_slim  ## this is the tf-slim (contrib.slim)\n",
    "slim = contrib_slim\n",
    "from slim.nets import resnet_utils  ## the is the slim image classification library from model/research/slim\n",
    "\n",
    "from lib import dataloader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_to_num_classes {'semantic': 3}\n",
      "crop_size [256, 256]\n",
      "atrous_rates [2, 4, 6]\n",
      "output_stride 8\n",
      "preprocessed_images_dtype <dtype: 'float32'>\n",
      "merge_method max\n",
      "add_image_level_feature True\n",
      "image_pooling_crop_size None\n",
      "image_pooling_stride [1, 1]\n",
      "aspp_with_batch_norm True\n",
      "aspp_with_separable_conv False\n",
      "multi_grid [1, 2]\n",
      "decoder_output_stride [4, 2]\n",
      "decoder_use_separable_conv False\n",
      "logits_kernel_size 1\n",
      "model_variant resnet_mod\n",
      "depth_multiplier 1.0\n",
      "divisible_by None\n",
      "prediction_with_upsampled_logits True\n",
      "dense_prediction_cell_config None\n",
      "nas_architecture_options {'nas_stem_output_num_conv_filters': 20, 'nas_use_classification_head': False, 'nas_remove_os32_stride': False}\n",
      "use_bounded_activation False\n",
      "aspp_with_concat_projection True\n",
      "aspp_with_squeeze_and_excitation False\n",
      "aspp_convs_filters 256\n",
      "decoder_use_sum_merge False\n",
      "decoder_filters 256\n",
      "decoder_output_is_logits False\n",
      "image_se_uses_qsigmoid False\n",
      "label_weights 1.0\n",
      "sync_batch_norm_method None\n",
      "batch_norm_decay 0.9997\n"
     ]
    }
   ],
   "source": [
    "crop_size = [256, 256]\n",
    "outputs_to_num_classes = {'semantic': 3}\n",
    "\n",
    "model_options = common.ModelOptions(\n",
    "    outputs_to_num_classes,\n",
    "    crop_size,\n",
    "    output_stride=8\n",
    ")._replace(\n",
    "    add_image_level_feature=True,\n",
    "    aspp_with_batch_norm=True,\n",
    "    aspp_with_separable_conv=False,\n",
    "    decoder_use_separable_conv=False,\n",
    "    logits_kernel_size=1,\n",
    "    decoder_output_stride=[4,2],\n",
    "    multi_grid=[1,2],\n",
    "    atrous_rates=[2,4,6],\n",
    "    model_variant='resnet_mod') \n",
    "\n",
    "for k, v in model_options._asdict().items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "weight_decay=0.0001\n",
    "is_training=True\n",
    "fine_tune_batch_norm = True\n",
    "nas_training_hyper_parameters = None\n",
    "\n",
    "output_stride=model_options.output_stride,\n",
    "multi_grid=model_options.multi_grid,\n",
    "depth_multiplier=model_options.depth_multiplier,\n",
    "divisible_by=model_options.divisible_by,\n",
    "weight_decay=weight_decay,\n",
    "is_training=is_training,\n",
    "preprocessed_images_dtype=model_options.preprocessed_images_dtype,\n",
    "fine_tune_batch_norm=fine_tune_batch_norm,\n",
    "nas_architecture_options=model_options.nas_architecture_options,\n",
    "nas_training_hyper_parameters=nas_training_hyper_parameters,\n",
    "use_bounded_activation=model_options.use_bounded_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "preprocess_images=True,\n",
    "preprocessed_images_dtype=tf.float32\n",
    "global_pool=False\n",
    "num_classes = None\n",
    "name_scope = {\n",
    "    'mobilenet_v2': 'MobilenetV2',\n",
    "    'mobilenet_edgetpu': 'MobilenetEdgeTPU',\n",
    "    'mobilenet_v3_large_seg': 'MobilenetV3',\n",
    "    'mobilenet_v3_small_seg': 'MobilenetV3',\n",
    "    'resnet_v1_18': 'resnet_v1_18',\n",
    "    'resnet_v1_18_beta': 'resnet_v1_18',\n",
    "    'resnet_v1_50': 'resnet_v1_50',\n",
    "    'resnet_v1_50_beta': 'resnet_v1_50',\n",
    "    'resnet_v1_101': 'resnet_v1_101',\n",
    "    'resnet_v1_101_beta': 'resnet_v1_101',\n",
    "    'xception_41': 'xception_41',\n",
    "    'xception_65': 'xception_65',\n",
    "    'xception_71': 'xception_71',\n",
    "    'nas_pnasnet': 'pnasnet',\n",
    "    'nas_hnasnet': 'hnasnet',\n",
    "    'resnet_mod' : 'resnet_mod',\n",
    "    'resnet_v2_50' : 'resnet_v2_50',\n",
    "}\n",
    "model_variant = model_options.model_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom resnet feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from deeplab.core import resnet_v1_beta\n",
    "# from deeplab.core import utils\n",
    "# from deeplab.core import conv2d_ws\n",
    "# from tensorflow.contrib.slim.nets import resnet_utils\n",
    "# import functools\n",
    "\n",
    "# def resnet_mod_a(inputs,\n",
    "#                num_classes=None,\n",
    "#                is_training=None,\n",
    "#                global_pool=False,\n",
    "#                output_stride=None,\n",
    "#                multi_grid=None,\n",
    "#                root_depth_multiplier=0.25,\n",
    "#                reuse=None,\n",
    "#                scope='resnet_123'):\n",
    "#     \"\"\"\n",
    "#     A custom Resnet v1 18 beta variant.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ## define the multi_grid/atrous blocks\n",
    "#     if multi_grid is None:\n",
    "#         multi_grid = [1,1]\n",
    "#     else:\n",
    "#         if len(multi_grid) != 2:\n",
    "#           raise ValueError('Expect multi_grid to have length 2.')\n",
    "\n",
    "#     block4_args = []\n",
    "#     for rate in multi_grid:\n",
    "#         block4_args.append({'depth': 512, 'stride': 1, 'unit_rate': rate})\n",
    "\n",
    "#     blocks = [\n",
    "#       resnet_v1_beta.resnet_v2_small_beta_block(\n",
    "#           'block1', base_depth=64, num_units=1, stride=2),\n",
    "#       resnet_v1_beta.resnet_v2_small_beta_block(\n",
    "#           'block2', base_depth=128, num_units=1, stride=2),\n",
    "#       resnet_v1_beta.resnet_v2_small_beta_block(\n",
    "#           'block3', base_depth=256, num_units=1, stride=2),\n",
    "#       resnet_utils.Block('block4', resnet_v1_beta.lite_bottleneck_v2, block4_args),\n",
    "#     ]\n",
    "    \n",
    "#     root_block_fn = functools.partial(conv2d_ws.conv2d_same,\n",
    "#                                       num_outputs=32,\n",
    "#                                       kernel_size=3,\n",
    "#                                       stride=1,\n",
    "#                                       scope='root_conv1')\n",
    "\n",
    "#     batch_norm = utils.get_batch_norm_fn(sync_batch_norm_method)\n",
    "#     with tf.variable_scope(scope, 'bbb', [inputs], reuse=reuse) as sc:\n",
    "#         end_points_collection = sc.original_name_scope + '_end_points'\n",
    "#         with slim.arg_scope([\n",
    "#             conv2d_ws.conv2d, resnet_utils.stack_blocks_dense, resnet_v1_beta.lite_bottleneck_v2 \n",
    "#             ],outputs_collections=end_points_collection):\n",
    "#             if is_training is not None:\n",
    "#                 arg_scope = slim.arg_scope([batch_norm], is_training=is_training)\n",
    "#             else:\n",
    "#                 arg_scope = slim.arg_scope([])\n",
    "#             with arg_scope:\n",
    "#                 print(arg_scope)\n",
    "#                 net = inputs\n",
    "#                 if output_stride is not None:\n",
    "#                   if output_stride % 4 != 0:\n",
    "#                     raise ValueError('The output_stride needs to be a multiple of 4.')\n",
    "#     #               output_stride //= 4\n",
    "#                 net = root_block_fn(net)\n",
    "#     #             net = slim.max_pool2d(net, 3, stride=2, padding='SAME', scope='pool1')\n",
    "#                 net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n",
    "#                 ## add a batchnorm and relu layer since the last conv output don't have them in v2\n",
    "#                 net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
    "#                 # Convert end_points_collection into a dictionary of end_points.\n",
    "#                 end_points = slim.utils.convert_collection_to_dict(\n",
    "#                     end_points_collection)\n",
    "\n",
    "#                 return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "arg_scope = resnet_utils.resnet_arg_scope(\n",
    "        weight_decay=weight_decay,\n",
    "        batch_norm_decay=0.95,\n",
    "        batch_norm_epsilon=1e-5,\n",
    "        batch_norm_scale=True)\n",
    "\n",
    "def _preprocess_zero_mean_unit_range(inputs, dtype=tf.float32):\n",
    "  \"\"\"Map image values from [0, 255] to [-1, 1].\"\"\"\n",
    "  preprocessed_inputs = (2.0 / 255.0) * tf.to_float(inputs) - 1.0\n",
    "  return tf.cast(preprocessed_inputs, dtype=dtype)\n",
    "\n",
    "\n",
    "func = resnet_v1_beta.resnet_mod\n",
    "# func = resnet_mod_a\n",
    "preprocess_function = _preprocess_zero_mean_unit_range\n",
    "\n",
    "@functools.wraps(func)\n",
    "def network_fn(inputs, *args, **kwargs):\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        return func(preprocess_function(inputs, preprocessed_images_dtype),\n",
    "              *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create input and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(crop_size[0], crop_size[1], 1), name='input_layer')\n",
    "with tf.variable_scope('resnet_mod'):\n",
    "    features, end_points = network_fn(inputs=inputs,\n",
    "                                  num_classes=num_classes,\n",
    "                                  is_training=True,\n",
    "                                  global_pool=global_pool,\n",
    "                                  output_stride=model_options.output_stride,\n",
    "                                  multi_grid=model_options.multi_grid,\n",
    "                                  reuse=tf.AUTO_REUSE,\n",
    "                                  scope=name_scope[model_variant])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup ASPP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reuse = tf.AUTO_REUSE\n",
    "slim = contrib_slim\n",
    "\n",
    "LOGITS_SCOPE_NAME = 'logits'\n",
    "MERGED_LOGITS_SCOPE = 'merged_logits'\n",
    "IMAGE_POOLING_SCOPE = 'image_pooling'\n",
    "ASPP_SCOPE = 'aspp'\n",
    "CONCAT_PROJECTION_SCOPE = 'concat_projection'\n",
    "DECODER_SCOPE = 'decoder'\n",
    "META_ARCHITECTURE_SCOPE = 'meta_architecture'\n",
    "\n",
    "PROB_SUFFIX = '_prob'\n",
    "\n",
    "_resize_bilinear = utils.resize_bilinear\n",
    "scale_dimension = utils.scale_dimension\n",
    "split_separable_conv2d = utils.split_separable_conv2d\n",
    "\n",
    "\n",
    "batch_norm_params = utils.get_batch_norm_params(\n",
    "                      decay=0.9997,\n",
    "                      epsilon=1e-5,\n",
    "                      scale=True,\n",
    "                      is_training=(is_training and fine_tune_batch_norm),\n",
    "#                       is_training=None,\n",
    "                      sync_batch_norm_method=model_options.sync_batch_norm_method)\n",
    "batch_norm = utils.get_batch_norm_fn(\n",
    "                      model_options.sync_batch_norm_method)\n",
    "activation_fn = tf.nn.relu\n",
    "\n",
    "batch_norm_params['is_training'] = tf.placeholder(tf.bool, name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with slim.arg_scope(\n",
    "          [slim.conv2d, slim.separable_conv2d],\n",
    "          weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "          activation_fn=activation_fn,\n",
    "          normalizer_fn=batch_norm,\n",
    "          padding='SAME',\n",
    "          stride=1,\n",
    "          reuse=reuse):\n",
    "    with slim.arg_scope([batch_norm], **batch_norm_params):\n",
    "        depth = model_options.aspp_convs_filters\n",
    "        branch_logits = []\n",
    "\n",
    "        if model_options.add_image_level_feature:\n",
    "            if model_options.crop_size is not None:\n",
    "                image_pooling_crop_size = model_options.image_pooling_crop_size\n",
    "              # If image_pooling_crop_size is not specified, use crop_size.\n",
    "                if image_pooling_crop_size is None:\n",
    "                    image_pooling_crop_size = model_options.crop_size\n",
    "                pool_height = scale_dimension(\n",
    "                                image_pooling_crop_size[0],\n",
    "                                1. / model_options.output_stride)\n",
    "                pool_width = scale_dimension(\n",
    "                                image_pooling_crop_size[1],\n",
    "                                1. / model_options.output_stride)\n",
    "                image_feature = slim.avg_pool2d(\n",
    "                                    features, [pool_height, pool_width],\n",
    "                                    model_options.image_pooling_stride, padding='VALID')\n",
    "                resize_height = scale_dimension(\n",
    "                                  model_options.crop_size[0],\n",
    "                                  1. / model_options.output_stride)\n",
    "                resize_width = scale_dimension(\n",
    "                                  model_options.crop_size[1],\n",
    "                                  1. / model_options.output_stride)\n",
    "            else:\n",
    "              # If crop_size is None, we simply do global pooling.\n",
    "                pool_height = tf.shape(features)[1]\n",
    "                pool_width = tf.shape(features)[2]\n",
    "                image_feature = tf.reduce_mean(\n",
    "                                    features, axis=[1, 2], keepdims=True)\n",
    "                resize_height = pool_height\n",
    "                resize_width = pool_width\n",
    "            image_feature_activation_fn = tf.nn.relu\n",
    "            image_feature_normalizer_fn = batch_norm\n",
    "            \n",
    "            \n",
    "            image_feature = slim.conv2d(\n",
    "                                image_feature, depth, 1,\n",
    "                                activation_fn=image_feature_activation_fn,\n",
    "                                normalizer_fn=image_feature_normalizer_fn,\n",
    "                                scope=IMAGE_POOLING_SCOPE)\n",
    "            image_feature = _resize_bilinear(\n",
    "                                image_feature,\n",
    "                                [resize_height, resize_width],\n",
    "                                image_feature.dtype)\n",
    "            # Set shape for resize_height/resize_width if they are not Tensor.\n",
    "            if isinstance(resize_height, tf.Tensor):\n",
    "              resize_height = None\n",
    "            if isinstance(resize_width, tf.Tensor):\n",
    "              resize_width = None\n",
    "            image_feature.set_shape([None, resize_height, resize_width, depth])\n",
    "            if not model_options.aspp_with_squeeze_and_excitation:\n",
    "                branch_logits.append(image_feature)\n",
    "\n",
    "        # Employ a 1x1 convolution.\n",
    "        branch_logits.append(slim.conv2d(features, depth, 1,\n",
    "                                       scope=ASPP_SCOPE + str(0)))\n",
    "        if model_options.atrous_rates:\n",
    "            # Employ 3x3 convolutions with different atrous rates.\n",
    "            for i, rate in enumerate(model_options.atrous_rates, 1):\n",
    "                scope = ASPP_SCOPE + str(i)\n",
    "                if model_options.aspp_with_separable_conv:\n",
    "                    aspp_features = split_separable_conv2d(\n",
    "                                        features,\n",
    "                                        filters=depth,\n",
    "                                        rate=rate,\n",
    "                                        weight_decay=weight_decay,\n",
    "                                        scope=scope)\n",
    "                else:\n",
    "                    aspp_features = slim.conv2d(\n",
    "                        features, depth, 3, rate=rate, scope=scope)\n",
    "                branch_logits.append(aspp_features)\n",
    "        \n",
    "        # Merge branch logits.\n",
    "        concat_logits = tf.concat(branch_logits, 3)\n",
    "        if model_options.aspp_with_concat_projection:\n",
    "            concat_logits = slim.conv2d(\n",
    "                            concat_logits, depth, 1, scope=CONCAT_PROJECTION_SCOPE)\n",
    "            concat_logits = slim.dropout(\n",
    "                            concat_logits,\n",
    "                            keep_prob=0.9,\n",
    "#                             is_training=is_training,\n",
    "                            scope=CONCAT_PROJECTION_SCOPE + '_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(512)]),\n",
       " 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_width, features.shape, model_options.output_stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model_options = model_options._replace(\n",
    "# decoder_output_stride = [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 2], [256, 256], False, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options.decoder_output_stride, model_options.crop_size, model_options.decoder_use_separable_conv, model_options.decoder_use_sum_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, False, 'resnet_mod', {'semantic': 3})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options.decoder_filters, model_options.decoder_output_is_logits, model_options.model_variant, model_options.outputs_to_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 4, 6], 1, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options.atrous_rates, model_options.logits_kernel_size, model_options.prediction_with_upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "is_training=True\n",
    "training = tf.placeholder(tf.bool, name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length feature list:1\n",
      "length decoder feature list:1\n",
      "length decoder feature list:2\n",
      "(?, 64, 64, 256)\n",
      "length feature list:1\n",
      "length decoder feature list:1\n",
      "length decoder feature list:2\n",
      "(?, 128, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "crop_size = model_options.crop_size\n",
    "    \n",
    "decoded = model.refine_by_decoder(\n",
    "            concat_logits,\n",
    "            end_points,\n",
    "            crop_size=crop_size,\n",
    "            decoder_output_stride=model_options.decoder_output_stride,\n",
    "            decoder_use_separable_conv=model_options.decoder_use_separable_conv,\n",
    "            decoder_use_sum_merge=model_options.decoder_use_sum_merge,\n",
    "            decoder_filters=model_options.decoder_filters,\n",
    "            decoder_output_is_logits=model_options.decoder_output_is_logits,\n",
    "            model_variant=model_options.model_variant,\n",
    "            weight_decay=weight_decay,\n",
    "            reuse=reuse,\n",
    "            is_training=is_training,\n",
    "            fine_tune_batch_norm=False,\n",
    "            use_bounded_activation=model_options.use_bounded_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(128), Dimension(128), Dimension(256)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semantic': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_options.outputs_to_num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(128), Dimension(128), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.get_branch_logits(\n",
    "                              decoded,\n",
    "                              model_options.outputs_to_num_classes['semantic'],\n",
    "                              model_options.atrous_rates,\n",
    "                              aspp_with_batch_norm=model_options.aspp_with_batch_norm,\n",
    "                              kernel_size=model_options.logits_kernel_size,\n",
    "                              weight_decay=weight_decay,\n",
    "                              reuse=reuse,\n",
    "                              scope_suffix='')\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from deeplab.core.utils import resize_bilinear\n",
    "\n",
    "outputs_scaled = resize_bilinear(outputs,\n",
    "                        crop_size,\n",
    "                        tf.float32)\n",
    "# MERGED_LOGITS_SCOPE = 'merged_logits'\n",
    "# outputs_to_scales_to_logits = {\n",
    "#       k: {}\n",
    "#       for k in model_options.outputs_to_num_classes\n",
    "# }\n",
    "# outputs_to_scales_to_logits[output][MERGED_LOGITS_SCOPE] = outputs_to_logits[output]\n",
    "\n",
    "# scales_to_logits = outputs_to_scales_to_logits[output]\n",
    "# logits = scales_to_logits[MERGED_LOGITS_SCOPE]\n",
    "# logits = resize_bilinear(outputs_to_logits[output],\n",
    "#                         tf.shape(inputs)[1:3],\n",
    "#                         scales_to_logits[MERGED_LOGITS_SCOPE].dtype)\n",
    "# predictions = tf.argmax(logits, 3)\n",
    "# predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run session graph, write summary for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256, 256, 1)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "g = tf.get_default_graph()\n",
    "with g.as_default():\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        img = tf.random_uniform(\n",
    "                (4, crop_size[0], crop_size[1], 1))\n",
    "        print(img.shape)\n",
    "        print(img.dtype)\n",
    "#         outputs_to_scales_to_logits = model.multi_scale_logits(\n",
    "#                                         inputs,\n",
    "#                                         model_options,\n",
    "#                                         image_pyramid=[1.0])\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         outputs_to_scales_to_logits = sess.run(outputs_to_logits, feed_dict={'input_layer:0': img.eval(), 'training:0':True})\n",
    "        outputs = sess.run(outputs_scaled, feed_dict={'input_layer:0': img.eval(), 'training:0':True})\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"output2/resnet_mod_v2_decode4-2\", sess.graph)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "outputs_to_scales_to_logits['semantic'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
